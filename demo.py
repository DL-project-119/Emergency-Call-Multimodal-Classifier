import os
import json
import torch
import librosa
import numpy as np
import whisper
import google.generativeai as genai
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline
import torch.nn as nn
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

SR = 16000
N_MFCC = 40
TARGET_T = 300

genai.configure(api_key="API_KEY")  
GEMINI_MODEL = "models/gemini-2.5-flash"
location_model = genai.GenerativeModel(GEMINI_MODEL)
summary_model = genai.GenerativeModel(GEMINI_MODEL)

# Whisper STT
whisper_model = whisper.load_model("large-v3")

def run_stt(wav_path):
    result = whisper_model.transcribe(wav_path, fp16=True)
    return result["text"].strip()


# ELECTRA tokenizer (ÌÖçÏä§Ìä∏ ÏûÖÎ†• Ï≤òÎ¶¨)
tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")


# MFCC ÌäπÏßï Ï∂îÏ∂ú
def extract_mfcc_sequence(wav_path, sr=SR, n_mfcc=N_MFCC, target_t=TARGET_T):
    try:
        y, _ = librosa.load(wav_path, sr=sr)

        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)

        mean = mfcc.mean(axis=1, keepdims=True)
        std  = mfcc.std(axis=1, keepdims=True) + 1e-6
        mfcc = (mfcc - mean) / std

        T_orig = mfcc.shape[1]
        if T_orig < 2:
            return np.zeros((target_t, n_mfcc), dtype=np.float32)

        old_x = np.linspace(0, 1, T_orig)
        new_x = np.linspace(0, 1, target_t)

        resampled = np.zeros((n_mfcc, target_t), dtype=np.float32)
        for i in range(n_mfcc):
            resampled[i] = np.interp(new_x, old_x, mfcc[i])

        return resampled.T.astype(np.float32)

    except Exception as e:
        print("[ERROR] Feature extraction failed:", e)
        return np.zeros((target_t, n_mfcc), dtype=np.float32)


# ÌÖçÏä§Ìä∏ + Ïò§ÎîîÏò§ ÏûÖÎ†• ÏÉùÏÑ±
def make_inputs(text, audio_mfcc, tokenizer):
    enc = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=512,
        return_tensors="pt"
    )

    input_ids = enc["input_ids"]
    attention_mask = enc["attention_mask"]
    token_type_ids = enc.get("token_type_ids", torch.zeros_like(input_ids))

    audio_tensor = torch.tensor(audio_mfcc, dtype=torch.float32).unsqueeze(0)

    return input_ids, attention_mask, token_type_ids, audio_tensor


# Electra + BiLSTM Î©ÄÌã∞Î™®Îã¨ Î™®Îç∏ Ï†ïÏùò
class MultiModalBiLSTMClassifier(nn.Module):
    def __init__(self, num_major, num_urg, freeze_electra=True):
        super().__init__()

        self.text_encoder = AutoModel.from_pretrained("beomi/KcELECTRA-base")
        hidden_size = self.text_encoder.config.hidden_size  # 768

        if freeze_electra:
            for p in self.text_encoder.parameters():
                p.requires_grad = False

        self.audio_lstm = nn.LSTM(
            input_size=40,
            hidden_size=128,
            num_layers=1,
            batch_first=True,
            bidirectional=True,
        )

        self.audio_bn = nn.BatchNorm1d(256)
        self.audio_dropout = nn.Dropout(0.3)

        self.fusion_fc = nn.Sequential(
            nn.Linear(hidden_size + 256, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
        )

        self.major_head = nn.Linear(512, num_major)
        self.urg_head   = nn.Linear(512, num_urg)

    def forward(self, input_ids, attention_mask, token_type_ids, audio):
        text_out = self.text_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
        )
        cls_vec = text_out.last_hidden_state[:, 0, :]

        audio_out, _ = self.audio_lstm(audio)
        audio_vec = audio_out[:, -1, :]
        audio_vec = self.audio_bn(audio_vec)
        audio_vec = self.audio_dropout(audio_vec)

        fused = torch.cat([cls_vec, audio_vec], dim=1)
        fused = self.fusion_fc(fused)

        major_logits = self.major_head(fused)
        urg_logits   = self.urg_head(fused)

        return major_logits, urg_logits


# Î©ÄÌã∞Î™®Îã¨ Î™®Îç∏ Î°úÎìú
num_major_classes = 4
num_urg_classes = 3

model = MultiModalBiLSTMClassifier(
    num_major=num_major_classes,
    num_urg=num_urg_classes,
    freeze_electra=False
)
model.load_state_dict(torch.load(
    "./model/finetuned_electra_bilstm.pt",
    map_location=device
))
model.to(device)
model.eval()


# ÏúÑÏπò Ï∂îÏ∂ú
def extract_location_with_llm(stt_text):
    prompt = f"""
Îã§Ïùå STTÏóêÏÑú Ïã§Ï†ú 'ÏúÑÏπò Ï†ïÎ≥¥'Îßå Î™®Îëê Ï∂îÏ∂úÌïòÏÑ∏Ïöî.
Ï∂úÎ†• ÌòïÏãùÏùÄ Î∞òÎìúÏãú JSON Î∞∞Ïó¥Îßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî.

STT:
{stt_text}
"""
    response = location_model.generate_content(prompt)
    text = response.text.strip()

    try:
        return json.loads(text)
    except:
        return []


# ÏöîÏïΩ ÏÉùÏÑ±
def summarize_with_llm(stt_text, major, urgency, locations):
    if isinstance(locations, list):
        location_text = ", ".join(locations) if locations else "ÏóÜÏùå"

    prompt = f"""
[STT]
{stt_text}

[Î™®Îç∏ Î∂ÑÏÑù Í≤∞Í≥º]
- ÏÉÅÌô©: {major}
- Í∏¥Í∏âÎèÑ: {urgency}
- ÏúÑÏπò: {location_text}

ÏïÑÎûò ÌòïÏãùÏúºÎ°ú 119 ÏÉÅÌô© ÏöîÏïΩÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî:

=== ÏÉÅÌô© ÏöîÏïΩ ===
(ÌïµÏã¨ ÏÇ¨Í±¥ 2~4Ï§Ñ)

=== ÎåÄÏùë ÌïÑÏöîÏÑ± ÌåêÎã® ===
(Ïôú Ï∂úÎèôÌï¥Ïïº ÌïòÎäîÏßÄ, ÏúÑÌóòÏÑ± Ï§ëÏã¨)

=== Ï∂úÎèô ÏöîÏïΩ Î©îÏãúÏßÄ ===
(ÏÉÅÌô©Ïã§ Ï†ÑÎã¨Ïö© Ìïú Î¨∏Ïû•)
"""
    response = summary_model.generate_content(prompt)
    return response.text.strip()


# Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
def predict_pipeline(wav_path):
    print("üé§ STT Î≥ÄÌôò Ï§ë...")
    stt_text = run_stt(wav_path)

    print("üìç ÏúÑÏπò Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë...")
    locations = extract_location_with_llm(stt_text)

    print("üéß MFCC Ï∂îÏ∂ú Ï§ë...")
    audio_mfcc = extract_mfcc_sequence(wav_path)

    print("‚öô ÏûÖÎ†• Íµ¨ÏÑ± Ï§ë...")
    input_ids, attention_mask, token_type_ids, audio_tensor = make_inputs(
        stt_text, audio_mfcc, tokenizer
    )

    input_ids      = input_ids.to(device)
    attention_mask = attention_mask.to(device)
    token_type_ids = token_type_ids.to(device)
    audio_tensor   = audio_tensor.to(device)

    print("ü§ñ ÏÉÅÌô© Î∂ÑÎ•ò Î™®Îç∏ Ï∂îÎ°† Ï§ë...")
    with torch.no_grad():
        major_logit, urg_logit = model(
            input_ids, attention_mask, token_type_ids, audio_tensor
        )

    major_idx = torch.argmax(major_logit, 1).item()
    urg_idx   = torch.argmax(urg_logit, 1).item()

    major_map = {0: "Íµ¨Í∏â", 1: "Íµ¨Ï°∞", 2: "ÌôîÏû¨", 3: "Í∏∞ÌÉÄ"}
    urg_map   = {0: "Ìïò", 1: "Ï§ë", 2: "ÏÉÅ"}

    major = major_map.get(major_idx, "N/A")
    urgency = urg_map.get(urg_idx, "N/A")

    print("üß† Ï¢ÖÌï© ÏöîÏïΩ ÏÉùÏÑ± Ï§ë...")
    summary = summarize_with_llm(stt_text, major, urgency, locations)

    return {
        "text": stt_text,
        "major": major,
        "urgency": urgency,
        "locations": locations,
        "llm_summary": summary
    }


# Ï∂úÎ†• Ìè¨Îß∑
def print_result(result):
    print("\n=================== üÜò 119 Ïã†Í≥† Î∂ÑÏÑù Í≤∞Í≥º ===================")

    print("\nüìÑ STT Ï∂îÏ∂ú ÎÇ¥Ïö©")
    print("-" * 60)
    print(result["text"])

    print("\nüß≠ Î™®Îç∏ Î∂ÑÎ•ò Í≤∞Í≥º")
    print("-" * 60)
    print(f"‚Ä¢ ÏÉÅÌô© Î∂ÑÎ•ò: {result['major']}")
    print(f"‚Ä¢ Í∏¥Í∏âÎèÑ:   {result['urgency']}")

    print("\nüß† LLM Ï¢ÖÌï© ÏöîÏïΩ")
    print("-" * 60)
    print(result["llm_summary"])

    print("\n============================================================\n")


if __name__ == "__main__":
    # result = predict_pipeline("./demo/2/64dd752b1ef84058319a7fd1_20230212123359.wav")
    # print_result(result)
    
    result = predict_pipeline("./demo/2/64d9fdff3e12da15ae3a5940_20230211201601.wav")
    print_result(result)
    
    # result = predict_pipeline("./demo/2/6551fb0dd9c67ad7fa18a6fc_20220228.wav")
    # print_result(result)
